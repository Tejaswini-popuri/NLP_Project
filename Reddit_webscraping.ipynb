{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d8770b",
   "metadata": {},
   "source": [
    "# Code to webscrape data from Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6800ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abaef308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613df34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'icA3VunYuiGWJM7Hywbd1w'\n",
    "client_secret = 'XVEBKYJgah2cA3vxdJixZtYX2Sm2kg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ec4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Scraper_v_1.0 by /u/tjpop\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id = client_id,\n",
    "    client_secret= client_secret,\n",
    "    user_agent = user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b55fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts1 = []\n",
    "posts2 = []\n",
    "posts3 = []\n",
    "subreddit1 = reddit.subreddit('datascience')\n",
    "subreddit2 = reddit.subreddit('machinelearning')\n",
    "subreddit3 = reddit.subreddit('artificial')\n",
    "for post in subreddit1.top(limit = None):\n",
    "    posts1.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "for post in subreddit2.top(limit = None):\n",
    "    posts2.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "for post in subreddit3.top(limit = None):\n",
    "    posts3.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "ds_posts = pd.DataFrame(posts1, columns=['id','title','subreddit','body','num_comments'])\n",
    "ml_posts = pd.DataFrame(posts2, columns=['id','title','subreddit','body','num_comments'])\n",
    "ai_posts = pd.DataFrame(posts3, columns=['id','title','subreddit','body','num_comments'])\n",
    "frames = [ds_posts,ml_posts,ai_posts]\n",
    "top_posts = pd.concat(frames,ignore_index=True)\n",
    "top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3055e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts1 = []\n",
    "posts2 = []\n",
    "posts3 = []\n",
    "subreddit1 = reddit.subreddit('datascience')\n",
    "subreddit2 = reddit.subreddit('machinelearning')\n",
    "subreddit3 = reddit.subreddit('artificial')\n",
    "for post in subreddit1.hot(limit = None):\n",
    "    posts1.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "for post in subreddit2.hot(limit = None):\n",
    "    posts2.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "for post in subreddit3.hot(limit = None):\n",
    "    posts3.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "ds_posts = pd.DataFrame(posts1, columns=['id','title','subreddit','body','num_comments'])\n",
    "ml_posts = pd.DataFrame(posts2, columns=['id','title','subreddit','body','num_comments'])\n",
    "ai_posts = pd.DataFrame(posts3, columns=['id','title','subreddit','body','num_comments'])\n",
    "frames = [ds_posts,ml_posts,ai_posts]\n",
    "hot_posts = pd.concat(frames,ignore_index=True)\n",
    "hot_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts1 = []\n",
    "posts2 = []\n",
    "posts3 = []\n",
    "subreddit1 = reddit.subreddit('datascience')\n",
    "subreddit2 = reddit.subreddit('machinelearning')\n",
    "subreddit3 = reddit.subreddit('artificial')\n",
    "for post in subreddit1.new(limit = None):\n",
    "    posts1.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "for post in subreddit2.new(limit = None):\n",
    "    posts2.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "for post in subreddit3.new(limit = None):\n",
    "    posts3.append([post.id,post.title,post.subreddit,post.selftext,post.num_comments])\n",
    "ds_posts = pd.DataFrame(posts1, columns=['id','title','subreddit','body','num_comments'])\n",
    "ml_posts = pd.DataFrame(posts2, columns=['id','title','subreddit','body','num_comments'])\n",
    "ai_posts = pd.DataFrame(posts3, columns=['id','title','subreddit','body','num_comments'])\n",
    "frames = [ds_posts,ml_posts,ai_posts]\n",
    "new_posts = pd.concat(frames,ignore_index=True)\n",
    "new_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921f3ee",
   "metadata": {},
   "source": [
    "reddit API only stores upto 2000 posts in each category(top, hot, new). The idea is to collect all the posts in three categories (i.e. top, new,hot) and drop any posts that repeated in all or more than one category and keep the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames = [hot_posts,top_posts,new_posts]\n",
    "posts = pd.concat(all_frames,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1527cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = posts.drop_duplicates(keep='first',ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[posts.subreddit == 'datascience'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[posts.subreddit == 'MachineLearning'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966de66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[posts.subreddit == 'artificial'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ce8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b7022",
   "metadata": {},
   "source": [
    "distribution looks pretty much even among the 3 subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d677dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv('C:/Users/Tejaswini/Documents/MSIS_COURSE/Coursework/Summer_2022/Self_Practice/redditnlp_0na.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f40f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('C:/Users/Tejaswini/Documents/MSIS_COURSE/Coursework/Summer_2022/Self_Practice/redditnlp_0na.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a260826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wopny7</td>\n",
       "      <td>Weekly Entering &amp; Transitioning - Thread 15 Au...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>\\n\\nWelcome to this week's entering &amp; transit...</td>\n",
       "      <td>50</td>\n",
       "      <td>Weekly Entering &amp; Transitioning - Thread 15 Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wqp4qm</td>\n",
       "      <td>When you are invited to a ‘town hall’ amidst a...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>When you are invited to a ‘town hall’ amidst a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrd1s7</td>\n",
       "      <td>Resume critique (trying to get an internship o...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>Resume critique (trying to get an internship o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrc8ad</td>\n",
       "      <td>Advice - Looking for a Data Scientist</td>\n",
       "      <td>datascience</td>\n",
       "      <td>Been trying to find a solid Data Scientist for...</td>\n",
       "      <td>51</td>\n",
       "      <td>Advice - Looking for a Data Scientist Been try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wriszy</td>\n",
       "      <td>Recommendations for Udemy class on deploying a...</td>\n",
       "      <td>datascience</td>\n",
       "      <td>I use Python mostly. Trying to move from analy...</td>\n",
       "      <td>0</td>\n",
       "      <td>Recommendations for Udemy class on deploying a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title    subreddit  \\\n",
       "0  wopny7  Weekly Entering & Transitioning - Thread 15 Au...  datascience   \n",
       "1  wqp4qm  When you are invited to a ‘town hall’ amidst a...  datascience   \n",
       "2  wrd1s7  Resume critique (trying to get an internship o...  datascience   \n",
       "3  wrc8ad              Advice - Looking for a Data Scientist  datascience   \n",
       "4  wriszy  Recommendations for Udemy class on deploying a...  datascience   \n",
       "\n",
       "                                                body  num_comments  \\\n",
       "0   \\n\\nWelcome to this week's entering & transit...            50   \n",
       "1                                                NaN            85   \n",
       "2                                                NaN            13   \n",
       "3  Been trying to find a solid Data Scientist for...            51   \n",
       "4  I use Python mostly. Trying to move from analy...             0   \n",
       "\n",
       "                                            all_text  \n",
       "0  Weekly Entering & Transitioning - Thread 15 Au...  \n",
       "1  When you are invited to a ‘town hall’ amidst a...  \n",
       "2  Resume critique (trying to get an internship o...  \n",
       "3  Advice - Looking for a Data Scientist Been try...  \n",
       "4  Recommendations for Udemy class on deploying a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
